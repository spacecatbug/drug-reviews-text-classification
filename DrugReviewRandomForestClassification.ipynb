{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DrugReviewRandomForestClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1L0vAsRRFuh_Nc1yURJ5a7aMB3xYHiObm",
      "authorship_tag": "ABX9TyMf6EyrOT4bUrLSauLaMl28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qma0jQoRCRsy",
        "colab_type": "text"
      },
      "source": [
        "## Initial data inspection\n",
        "Load in test and training data files. The data used for this analysis can be found [here](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Druglib.com%29).\n",
        "\n",
        "Optional: If working in Google Colab, use drive.\n",
        "mount() so that you can import files from Google Drive into your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCEyP7h2CQ_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optional data import from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "train_file = '/content/drive/My Drive/drugLib_raw/drugLibTrain_raw.tsv'\n",
        "test_file = '/content/drive/My Drive/drugLib_raw/drugLibTest_raw.tsv'\n",
        "\n",
        "train_df = pd.read_csv(train_file,sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt4XdBDJGbUt",
        "colab_type": "text"
      },
      "source": [
        "This dataset contains information describing the drug information, the condition that it being treated and the patient reviews. \n",
        "\n",
        "The aim is to predict the `effectiveness` from this dataset. \n",
        "\n",
        "The patient review data is divided into three columns: `benefitsReview`, `sideEffectsReview` and `commentsReview`. The text will be mined from these three columns to try and predict drug `effectiveness`.\n",
        "\n",
        "A more comprehensive description of the data can be found [here](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Druglib.com%29)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWeY9FztHKBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "dac869ee-8327-4aef-d1a4-39ae856894bf"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>effectiveness</th>\n",
              "      <th>sideEffects</th>\n",
              "      <th>condition</th>\n",
              "      <th>benefitsReview</th>\n",
              "      <th>sideEffectsReview</th>\n",
              "      <th>commentsReview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2202</td>\n",
              "      <td>enalapril</td>\n",
              "      <td>4</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>management of congestive heart failure</td>\n",
              "      <td>slowed the progression of left ventricular dys...</td>\n",
              "      <td>cough, hypotension , proteinuria, impotence , ...</td>\n",
              "      <td>monitor blood pressure , weight and asses for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3117</td>\n",
              "      <td>ortho-tri-cyclen</td>\n",
              "      <td>1</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>birth prevention</td>\n",
              "      <td>Although this type of birth control has more c...</td>\n",
              "      <td>Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...</td>\n",
              "      <td>I Hate This Birth Control, I Would Not Suggest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1146</td>\n",
              "      <td>ponstel</td>\n",
              "      <td>10</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>No Side Effects</td>\n",
              "      <td>menstrual cramps</td>\n",
              "      <td>I was used to having cramps so badly that they...</td>\n",
              "      <td>Heavier bleeding and clotting than normal.</td>\n",
              "      <td>I took 2 pills at the onset of my menstrual cr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3947</td>\n",
              "      <td>prilosec</td>\n",
              "      <td>3</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>acid reflux</td>\n",
              "      <td>The acid reflux went away for a few months aft...</td>\n",
              "      <td>Constipation, dry mouth and some mild dizzines...</td>\n",
              "      <td>I was given Prilosec prescription at a dose of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1951</td>\n",
              "      <td>lyrica</td>\n",
              "      <td>2</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>fibromyalgia</td>\n",
              "      <td>I think that the Lyrica was starting to help w...</td>\n",
              "      <td>I felt extremely drugged and dopey.  Could not...</td>\n",
              "      <td>See above</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                     commentsReview\n",
              "0        2202  ...  monitor blood pressure , weight and asses for ...\n",
              "1        3117  ...  I Hate This Birth Control, I Would Not Suggest...\n",
              "2        1146  ...  I took 2 pills at the onset of my menstrual cr...\n",
              "3        3947  ...  I was given Prilosec prescription at a dose of...\n",
              "4        1951  ...                                          See above\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai4AlIbmCxH3",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "The data needs to be pre-processed before inputing it into a model.\n",
        "A model will perform better if the input data consist of features that have a significant impact on what you are trying to predict and the amount of noise is minimized (data that is deemed insignificant). Several techniques are used here in this analysis exercise:\n",
        "\n",
        "1.   Data cleansing\n",
        "2.   Lemmatization\n",
        "3.   Removal of stop words\n",
        "4.   Using term frequency-inverse document frequency (TF-IDF)\n",
        "\n",
        "### 1. Token Normalisation\n",
        "\n",
        "The train and test data is currently in tab delimited format and will be converted into Pandas Dataframes. \n",
        "\n",
        "An additional column `combinedReview` has been added which contains all the review data from the 3 columns, concatenated.\n",
        "\n",
        "Another additional column `label` has been included in these Dataframes that assigns classification labels `effectiveness` as integer values so that they can be read later by the model.\n",
        "\n",
        "The `combinedReview` data will be cleaned to remove any special (invalid) characters, multiple spaces, numbers, escape characters and any words that are just 1 character long. The review text will also be case-folded (converted all to lower case) so that words that are spelled the same will be grouped together regardless of their upper/lower casing.\n",
        "\n",
        "### 2. Case Folding\n",
        "All words are converted to lower case so that words that are spelt the same can be matched. It is reasonable to assume that variations in casings will not impact the sentinment of the reviews.\n",
        "\n",
        "### 3. Lemmatization\n",
        "\n",
        "Lemmatization takes words and reduces them down to its base form i.e. its lemma. This helps to group together words that are similar and can be considered equivalent for the purposes of input features for a model. For example, the lemma of  `have` and `had` is `have`, so these words will be grouped together and treated the same.\n",
        "\n",
        "### 4. Stemming\n",
        "Porter's Algorithm is used for Stemming which is a more crude technique for grouping words that can be considered equivalent that were not already reduced by lemmatisation, for example the words \"compressed\" and \"compression\" have different lemmas but with stemming they can all be reduced to the same base, \"compress\". Plurals are also singularized if not already done so in lemmatization.\n",
        "\n",
        "### 5. POS Tagging\n",
        "POS tags are part of speech tags. Only nouns, adjectives, adverbs and adverbs are included for model inputs and all other words belonging to other POS types are considered less significant and are discarded (e.g. articles, conjunctions).\n",
        "\n",
        "\n",
        "### 5. Removal of stop words\n",
        "Stop words, or function words, are the most common words in a language. They typically do not provide a lot of information for text mining and are often excluded in text classification e.g. this, the, a, of. However, a lot of negation words are included in the stop words which can have great significant in sentiment analysis, for example, the sentiment of the phrases `very effective` and `not very effective` are contrasting, but the words `not` and `very` are considered stop words. So a customised list of selected stop words are excluded from the reviews to reduce the noise for the model inputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E9mobvQCwxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "dcbe5468-f70c-4db2-fa2e-0c8468f751f6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "labels_dict = {}\n",
        "for count,label in enumerate(train_df[\"effectiveness\"].unique()):\n",
        "  labels_dict[label] = count+1\n",
        "\n",
        "def build_dataframe(filepath):\n",
        "    # Loading the training data into a Pandas Dataframe\n",
        "    df = pd.read_csv(filepath, sep='\\t')\n",
        "    # Creating new columns for cleaned data\n",
        "    df[\"benefitsReview_cleaned\"] = np.nan\n",
        "    df[\"sideEffectsReview_cleaned\"] = np.nan\n",
        "    df[\"commentsReview_cleaned\"] = np.nan\n",
        "\n",
        "    columns_to_clean = {\"benefitsReview\": \"benefitsReview_cleaned\",\n",
        "                        \"sideEffectsReview\": \"sideEffectsReview_cleaned\",\n",
        "                        \"commentsReview\": \"commentsReview_cleaned\"}\n",
        "\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        for raw_column, cleaned_column in columns_to_clean.items():\n",
        "            drug_review = df.loc[row.Index, raw_column]\n",
        "            # If the row is empty, no need to process, continue on processing the next line\n",
        "            if pd.isnull(drug_review):\n",
        "                continue\n",
        "\n",
        "            drug_review = clean_review_text(drug_review)\n",
        "            drug_review = ' '.join(drug_review)\n",
        "\n",
        "            # Update the 'clean column' with the cleaned drug_review\n",
        "            df.loc[row.Index, cleaned_column] = drug_review\n",
        "\n",
        "    # Concatenating 3 columns of unstructured, descriptive customer review data into a single column in the Dataframe\n",
        "    df[\"combinedReview\"] = df[\"benefitsReview_cleaned\"].str.cat(df[\"sideEffectsReview_cleaned\"],\n",
        "                                                                             sep=\" \").str.cat(df[\"commentsReview_cleaned\"], sep=\" \")\n",
        "    return df\n",
        "\n",
        "# Function used to clean the test and training drug review data and returns a tokenized list of the words in their lemma form(root form)\n",
        "def clean_review_text(drug_review):\n",
        "    # Remove special characters\n",
        "    drug_review = re.sub('\\W', ' ', drug_review)\n",
        "    # Remove underscores\n",
        "    drug_review = re.sub('_', '', drug_review)\n",
        "    # Remove single characters\n",
        "    drug_review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', drug_review)\n",
        "    # Remove all numbers\n",
        "    drug_review = re.sub(\"\\d+\", \"\", drug_review)\n",
        "    # Remove single characters from the start\n",
        "    drug_review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', drug_review)\n",
        "    # Substituting multiple spaces with single space\n",
        "    drug_review = re.sub(r'\\s+', ' ', drug_review, flags=re.I)\n",
        "    # Removing prefixed 'b'\n",
        "    drug_review = re.sub(r'^b\\s+', '', drug_review)\n",
        "    # Remove any newline escape characters\n",
        "    drug_review = re.sub(\"\\n\", \"\", drug_review)\n",
        "    # Converting to Lowercase\n",
        "    drug_review = drug_review.lower()\n",
        "    # Convert all instances of side effect, plural and singular form, to one feature\n",
        "    if 'side effect' in drug_review:\n",
        "        # Feature engineer 'side effect' as one feature as this is very significant feature for sentiment analysis\n",
        "        drug_review=drug_review.replace('side effect', 'side_effect')\n",
        "    if 'side affect' in drug_review:\n",
        "        drug_review=drug_review.replace('side affect','side_effect')  # correct frequent misspellings of side effect as this is an important input feature\n",
        "    drug_review = drug_review.split()\n",
        "    # Singularize all plurals to singular words\n",
        "    drug_review = [singularize(word) for word in drug_review]\n",
        "    # Lemmatization: Replace each word in each text file with its lemma form (root form) and convert to present tense\n",
        "    drug_review = [WordNetLemmatizer().lemmatize(word,'v') for word in drug_review]\n",
        "    # Stemming: Use Porter's algorithm to stem words\n",
        "    ps = PorterStemmer()\n",
        "    drug_review = [ps.stem(word) for word in drug_review]\n",
        "    # Remove a subset of stop words that are useful for this sentiment analysis\n",
        "    # If the entire term is a stopword, then do not include in the target vocab/lexicon\n",
        "    # The stopwords were initially added for the purpose of finding n-grams with stopwords as the stopwords would\n",
        "    # have value within an n-gram e.g. \"very effective\" and \"much more\" contain stopwords but are valuable\n",
        "    # when part of n-grams\n",
        "    all_stopwords = stopwords.words('english')\n",
        "    custom_stopwords = []\n",
        "    exclusions = ['no','not','down','more','few','most','other','some','no','not','only','same','so','very','aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "    for stopword in all_stopwords:\n",
        "        if stopword not in exclusions:\n",
        "            custom_stopwords.append(stopword)\n",
        "\n",
        "    clean_drug_review = []\n",
        "\n",
        "    postags = nltk.pos_tag(drug_review)\n",
        "    for tag in postags:\n",
        "        word = tag[0]\n",
        "        pos = tag[1]\n",
        "        if (len(word) > 2 and (pos[:2] == \"NN\" or pos[0] == \"V\" or pos[:2] == \"RB\" or pos[0] == \"J\")) or word in exclusions:\n",
        "            clean_drug_review.append(word)\n",
        "\n",
        "    clean_drug_review = ' '.join(clean_drug_review)\n",
        "    clean_drug_review = clean_drug_review.replace('have have','have')\n",
        "    clean_drug_review = clean_drug_review.split()\n",
        "    return clean_drug_review\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Function used to clean the test and training drug review data and returns a tokenized list of the words in their lemma form(root form)\n",
        "def clean_review_text(drug_review):\n",
        "    # Remove special characters\n",
        "    drug_review = re.sub('\\W', ' ', drug_review)\n",
        "    # Remove underscores\n",
        "    drug_review = re.sub('_', '', drug_review)\n",
        "    # Remove single characters\n",
        "    drug_review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', drug_review)\n",
        "    # Remove all numbers\n",
        "    drug_review = re.sub(\"\\d+\", \"\", drug_review)\n",
        "    # Remove single characters from the start\n",
        "    drug_review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', drug_review)\n",
        "    # Substituting multiple spaces with single space\n",
        "    drug_review = re.sub(r'\\s+', ' ', drug_review, flags=re.I)\n",
        "    # Removing prefixed 'b'\n",
        "    drug_review = re.sub(r'^b\\s+', '', drug_review)\n",
        "    # Remove any newline escape characters\n",
        "    drug_review = re.sub(\"\\n\", \"\", drug_review)\n",
        "    # Converting to Lowercase\n",
        "    drug_review = drug_review.lower()\n",
        "    # Convert all instances of side effect, plural and singular form, to one feature\n",
        "    if 'side effect' in drug_review:\n",
        "        # Feature engineer 'side effect' as one feature as this is very significant feature for sentiment analysis\n",
        "        drug_review=drug_review.replace('side effect', 'side_effect')\n",
        "    if 'side affect' in drug_review:\n",
        "        drug_review=drug_review.replace('side affect','side_effect')  # correct frequent misspellings of side effect as this is an important input feature\n",
        "    drug_review = drug_review.split()\n",
        "    # Lemmatization: Replace each word in each text file with its lemma form (root form) and convert to present tense\n",
        "    drug_review = [WordNetLemmatizer().lemmatize(word,'v') for word in drug_review]\n",
        "    # Stemming: Use Porter's algorithm to stem words\n",
        "    ps = PorterStemmer()\n",
        "    drug_review = [ps.stem(word) for word in drug_review]\n",
        "    # Remove a subset of stop words that are useful for this sentiment analysis\n",
        "    # If the entire term is a stopword, then do not include in the target vocab/lexicon\n",
        "    # The stopwords were initially added for the purpose of finding n-grams with stopwords as the stopwords would\n",
        "    # have value within an n-gram e.g. \"very effective\" and \"much more\" contain stopwords but are valuable\n",
        "    # when part of n-grams\n",
        "    all_stopwords = stopwords.words('english')\n",
        "    custom_stopwords = []\n",
        "    exclusions = ['no','not','down','more','few','most','other','some','no','not','only','same','so','very','aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "    for stopword in all_stopwords:\n",
        "        if stopword not in exclusions:\n",
        "            custom_stopwords.append(stopword)\n",
        "\n",
        "    clean_drug_review = []\n",
        "\n",
        "    postags = nltk.pos_tag(drug_review)\n",
        "    for tag in postags:\n",
        "        word = tag[0]\n",
        "        pos = tag[1]\n",
        "        if (len(word) > 2 and (pos[:2] == \"NN\" or pos[0] == \"V\" or pos[:2] == \"RB\" or pos[0] == \"J\")) or word in exclusions:\n",
        "            clean_drug_review.append(word)\n",
        "\n",
        "    clean_drug_review = ' '.join(clean_drug_review)\n",
        "    clean_drug_review = clean_drug_review.replace('have have','have')\n",
        "    clean_drug_review = clean_drug_review.split()\n",
        "    return clean_drug_review\n",
        "\n",
        "\n",
        "train_df = build_dataframe(train_file)\n",
        "test_df = build_dataframe(test_file)\n",
        "\n",
        "# Remove any duplicate lines in the train dataset and also any rows with no patient review data (no input for the model)\n",
        "train_df.drop_duplicates()\n",
        "train_df.dropna(subset=[\"combinedReview\"],inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUdBLX-Bxrhs",
        "colab_type": "text"
      },
      "source": [
        "Viewing the updated, cleaned Dataframe for the training dataset which includes the new `combinedReview` column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1wX-o6qxpA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "fef3d70b-4e8a-4668-a665-a5a114d65fec"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>effectiveness</th>\n",
              "      <th>sideEffects</th>\n",
              "      <th>condition</th>\n",
              "      <th>benefitsReview</th>\n",
              "      <th>sideEffectsReview</th>\n",
              "      <th>commentsReview</th>\n",
              "      <th>benefitsReview_cleaned</th>\n",
              "      <th>sideEffectsReview_cleaned</th>\n",
              "      <th>commentsReview_cleaned</th>\n",
              "      <th>combinedReview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2202</td>\n",
              "      <td>enalapril</td>\n",
              "      <td>4</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>management of congestive heart failure</td>\n",
              "      <td>slowed the progression of left ventricular dys...</td>\n",
              "      <td>cough, hypotension , proteinuria, impotence , ...</td>\n",
              "      <td>monitor blood pressure , weight and asses for ...</td>\n",
              "      <td>slow progress leav ventricular dysfunct overt ...</td>\n",
              "      <td>cough hypotens proteinuria impot renal failur ...</td>\n",
              "      <td>monitor blood pressur weight ass resolut fluid</td>\n",
              "      <td>slow progress leav ventricular dysfunct overt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3117</td>\n",
              "      <td>ortho-tri-cyclen</td>\n",
              "      <td>1</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>birth prevention</td>\n",
              "      <td>Although this type of birth control has more c...</td>\n",
              "      <td>Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...</td>\n",
              "      <td>I Hate This Birth Control, I Would Not Suggest...</td>\n",
              "      <td>thi type birth control have more con help cram...</td>\n",
              "      <td>heavi cycl cramp hot flash fatigu long last cy...</td>\n",
              "      <td>hate thi birth control not suggest thi anyon</td>\n",
              "      <td>thi type birth control have more con help cram...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1146</td>\n",
              "      <td>ponstel</td>\n",
              "      <td>10</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>No Side Effects</td>\n",
              "      <td>menstrual cramps</td>\n",
              "      <td>I was used to having cramps so badly that they...</td>\n",
              "      <td>Heavier bleeding and clotting than normal.</td>\n",
              "      <td>I took 2 pills at the onset of my menstrual cr...</td>\n",
              "      <td>use have cramp so leav ball bed least day pons...</td>\n",
              "      <td>heavier bleed clot normal</td>\n",
              "      <td>take pill onset menstrual cramp then everi hou...</td>\n",
              "      <td>use have cramp so leav ball bed least day pons...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3947</td>\n",
              "      <td>prilosec</td>\n",
              "      <td>3</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>acid reflux</td>\n",
              "      <td>The acid reflux went away for a few months aft...</td>\n",
              "      <td>Constipation, dry mouth and some mild dizzines...</td>\n",
              "      <td>I was given Prilosec prescription at a dose of...</td>\n",
              "      <td>acid reflux away few month just few day drug h...</td>\n",
              "      <td>constip dri mouth some mild dizzi away medic s...</td>\n",
              "      <td>give prilosec prescript dose day medic take on...</td>\n",
              "      <td>acid reflux away few month just few day drug h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1951</td>\n",
              "      <td>lyrica</td>\n",
              "      <td>2</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>fibromyalgia</td>\n",
              "      <td>I think that the Lyrica was starting to help w...</td>\n",
              "      <td>I felt extremely drugged and dopey.  Could not...</td>\n",
              "      <td>See above</td>\n",
              "      <td>think lyrica start help pain side_effect just ...</td>\n",
              "      <td>felt extrem drug dopey not drive while thi med...</td>\n",
              "      <td>see abov</td>\n",
              "      <td>think lyrica start help pain side_effect just ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                     combinedReview\n",
              "0        2202  ...  slow progress leav ventricular dysfunct overt ...\n",
              "1        3117  ...  thi type birth control have more con help cram...\n",
              "2        1146  ...  use have cramp so leav ball bed least day pons...\n",
              "3        3947  ...  acid reflux away few month just few day drug h...\n",
              "4        1951  ...  think lyrica start help pain side_effect just ...\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA_-v8LVQo97",
        "colab_type": "text"
      },
      "source": [
        "### 6. Using term frequency-inverse document frequency (TF-IDF)\n",
        "The reviews contain a very large range of words and it would take a lot of computational power to include every single unique word when training a model. We can use term frequency-inverse document frequency (TF-IDF) which is a statistic that measures how important words are to a corpus (the set of all reviews). TF-IDF depends on how many times a word appears in a single document and also how many different documents that word appears in.\n",
        "\n",
        "Higher TF-IDF values are given to words that appear  frequently within a single document *AND* if that word appears in a smaller number of documents. This means that if these words are seen in a document, it will be easier to predict what classification that document belongs to, based on that word. Using intuition, a word like `the` would have a low TF-IDF because it would probably appear in all documents. A word like `headache` may have a high TF-IDF because it seems like a word that would appear in a smaller number of reviews, and intuitively it would seem that this word highly suggests that the drug review is negative, for example. A word that is completely unique and only appears once in all the reviews would have a low TF-IDF because it appears infrequently in a single document (an example of this could be a misspelled word, like `headahce`.\n",
        "\n",
        "\n",
        "\n",
        "Create the lexicon by calculating Term Frequency - Infrequent Document Frequency (TF-IDF). We will save the terms with the top 600 TF-IDF scores as the vocabulary chosen as input features for this model.\n",
        "\n",
        "### 7. Using n-grams\n",
        "\n",
        "Tri-grams and four-grams are phrases made up of three or four words. These are used instead of individual words as input features to the model as they provide more information about sentiment, e.g. an input feature `not longer feel symptoms` or `experienced slight headache` expresses more sentiment accurately than the single words `symptoms` and `headache`.\n",
        "\n",
        "### 8. Named Entities\n",
        "As one would expect from drug reviews, the text data consists of both descriptive information on the user's experience and also information relating to how the medicine was taken, which is not very informative for sentiment analysis. This includes information such as what time of day medicine was taken, if the medicine was taken for a few months, or if one pill was taken in the morning etc. A custom blacklist of words was created to exclude any terms with such words, such as: day, hour, once, twice, daily, take and pill.\n",
        "\n",
        "### 9. Feature construction\n",
        "\n",
        "It was found that the term `side effect` was common amongst high scoring TF-IDF terms. Thus these words were joined together as a single feature by clustering the two words `side effect` into one word, `side_effect`. Another predominant phrase that was clustered was `have had`. So all the terms `have`, `have had` and `had` all reduce down to the word `have` as this simplifies the input features whilst having no impact on sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8KLNYIWSaPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "9a8f9057-cee2-4b29-99c7-6d97412c5ec3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfidf_lexicon(train_df,size=400):\n",
        "    # Engineer two features side effect as one grouped feature\n",
        "    # Calculate Term Frequency-Infrequent document frequency (TF-IDF) for 1-3 ngrams\n",
        "    # (groups of 1 word, groups of 2 words and groups of 3 words)\n",
        "    tv = TfidfVectorizer(ngram_range = (3,4),\n",
        "                         sublinear_tf = True,\n",
        "                         max_features = int(size*2))\n",
        "    train_tv = tv.fit_transform(train_df['combinedReview'])\n",
        "    vocab = tv.get_feature_names()\n",
        "    dist = np.sum(train_tv, axis=0)\n",
        "    vocab_tfidf_df = pd.DataFrame(dist,columns = vocab)\n",
        "    sorted_tfidf_df = vocab_tfidf_df.sort_values(vocab_tfidf_df.first_valid_index(),axis=1,ascending=False)\n",
        "    i = 0\n",
        "    lexicon = []\n",
        "    custom_exclusions = [ 'day', 'hour','week', 'once', 'twice', 'daily', 'morning','year','month', 'take', 'medication','pill']\n",
        "    for (word, score) in sorted_tfidf_df.iteritems():\n",
        "        exclude = False\n",
        "        for exclusion in custom_exclusions:\n",
        "            if exclusion in word:\n",
        "                exclude = True\n",
        "        if not exclude:\n",
        "            lexicon.append((word, score.values[0]))\n",
        "\n",
        "\n",
        "    target_lexicon = []\n",
        "\n",
        "    for term in lexicon:\n",
        "        word = term[0]\n",
        "        if len(word) == 2:\n",
        "            target_lexicon.append(term)\n",
        "        else:\n",
        "            for term1 in lexicon:\n",
        "                word1 = term1[0]\n",
        "                if word!=word1 and word in word1:\n",
        "                    continue\n",
        "            target_lexicon.append(term)\n",
        "\n",
        "    target_lexicon = target_lexicon[:size]\n",
        "\n",
        "    return target_lexicon\n",
        "\n",
        "lexicon_size=200\n",
        "lexicon_tfidf = tfidf_lexicon(train_df,size=lexicon_size)\n",
        "lexicon = [l[0] for l in lexicon_tfidf]\n",
        "lexicon_df_demo = pd.DataFrame(lexicon_tfidf,columns=['Term','TF-IDF_Score'])\n",
        "print(\"Top 20 terms in the review data, ranked by TF-IDF scores\")\n",
        "lexicon_df_demo.head(20)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 20 terms in the review data, ranked by TF-IDF scores\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Term</th>\n",
              "      <th>TF-IDF_Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>have no side_effect</td>\n",
              "      <td>32.484035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>not experi ani</td>\n",
              "      <td>29.887380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>there no side_effect</td>\n",
              "      <td>29.035621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>not have ani</td>\n",
              "      <td>26.905450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lower blood pressur</td>\n",
              "      <td>24.509820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>no longer have</td>\n",
              "      <td>21.030435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>high blood pressur</td>\n",
              "      <td>20.466402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>thi drug have</td>\n",
              "      <td>20.066177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>have not have</td>\n",
              "      <td>19.148428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>have ani side_effect</td>\n",
              "      <td>18.759807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>experi ani side_effect</td>\n",
              "      <td>18.072637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>work veri well</td>\n",
              "      <td>17.861272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>no notic side_effect</td>\n",
              "      <td>16.925329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>wake middl night</td>\n",
              "      <td>15.956562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>no side_effect have</td>\n",
              "      <td>14.993722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>have not experi</td>\n",
              "      <td>12.951336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>night befor bed</td>\n",
              "      <td>12.770006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>first thing morn</td>\n",
              "      <td>12.688074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>not experi ani side_effect</td>\n",
              "      <td>12.552216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>have never have</td>\n",
              "      <td>12.457136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Term  TF-IDF_Score\n",
              "0          have no side_effect     32.484035\n",
              "1               not experi ani     29.887380\n",
              "2         there no side_effect     29.035621\n",
              "3                 not have ani     26.905450\n",
              "4          lower blood pressur     24.509820\n",
              "5               no longer have     21.030435\n",
              "6           high blood pressur     20.466402\n",
              "7                thi drug have     20.066177\n",
              "8                have not have     19.148428\n",
              "9         have ani side_effect     18.759807\n",
              "10      experi ani side_effect     18.072637\n",
              "11              work veri well     17.861272\n",
              "12        no notic side_effect     16.925329\n",
              "13            wake middl night     15.956562\n",
              "14         no side_effect have     14.993722\n",
              "15             have not experi     12.951336\n",
              "16             night befor bed     12.770006\n",
              "17            first thing morn     12.688074\n",
              "18  not experi ani side_effect     12.552216\n",
              "19             have never have     12.457136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUEo_uw-zo7K",
        "colab_type": "text"
      },
      "source": [
        "Building the classifications which maps distinct classes from the dataset to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvajqCiczzcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "024c9313-ce1c-4f2d-e4fc-87f28d07a10a"
      },
      "source": [
        "def build_classifications(df):\n",
        "    classification_names = list(df.effectiveness.unique())\n",
        "    classifications = {}\n",
        "    id = 1\n",
        "    for classification in classification_names:\n",
        "        classifications[classification] = id\n",
        "        id += 1\n",
        "    return classifications\n",
        "\n",
        "classifications = build_classifications(train_df)\n",
        "print(classifications)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Highly Effective': 1, 'Marginally Effective': 2, 'Ineffective': 3, 'Considerably Effective': 4, 'Moderately Effective': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInzKNaxz_eu",
        "colab_type": "text"
      },
      "source": [
        "## Text to Vector\n",
        "\n",
        "The reviews now need to be converted into a format so that a machine learning model can read it.\n",
        "\n",
        "The final feature set is converted into a vector representing the frequency of words from the targeted vocabulary that are present in each review. This vector representation is called a bag of words model because it only retains information of the frequency of terms, but does not retain its order. Since the lexicon is 200 words long, the vector representation for each review is also 200 numbers long. Taking the target vocabulary that is printed in the code block above, the first 3 vocab words are day, effect and side.\n",
        "If the review text is `I experience this side effect on a day to day basis` then the vector representation would be [2,1,2] because `day` appears twice and `side` and `effect` appear once only.\n",
        "\n",
        "*   `X_train` contains the vector representation of the reviews from the training dataset. The model will use this dataset to train.\n",
        "*   `y_train` represents the classification for that review. Each review is represented by one number. For example, if the `effectiveness` of a drug review is `Highly Effective`, then this is represented in the `y_train` dataset as `1`. There are 3097 reviews in the final training dataset, so `y_train` would simply be a list of 3097 numbers, ranging from 1 to 5 for the different classes.\n",
        "The model will use this dataset to train.\n",
        "\n",
        "*   `X_test` contains the vector representation of the reviews from the test dataset. After the model is trained, it will use this as input features to try and predict the effectiveness.\n",
        "*   `y_test` contains the actual effectiveness for the reviews in the test dataset. After the model has predicted the classifications, its guesses will be compared against this dataset to measure its accuracy, precision and recall later.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzGWVsNa0EPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1ea87598-a1ce-4648-80e9-8b998f6356e8"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def build_featureset(df,lexicon,classifications):\n",
        "    featureset = []\n",
        "\n",
        "    # N-gram terms from the lexicon kept as a separate list\n",
        "    ngrams = [ngram for ngram in lexicon if (len(ngram.split()) > 1)]\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        combined_drug_review = df.loc[row.Index, \"combinedReview\"]\n",
        "        # If there is no review available for a row, no need to process it, continue on processing the next line\n",
        "        if pd.isnull(combined_drug_review):\n",
        "            continue\n",
        "        word_tokens = word_tokenize(combined_drug_review.lower())\n",
        "        features = np.zeros(len(lexicon))\n",
        "        classification = classifications[row.effectiveness]\n",
        "        for i in range(len(word_tokens)):\n",
        "            # Check match to n-grams first\n",
        "            for ngram in ngrams:\n",
        "                if ngram in combined_drug_review:\n",
        "                    index_value = lexicon.index(ngram.lower())\n",
        "                    features[index_value] += 1\n",
        "\n",
        "            # Matching one word terms\n",
        "            if word_tokens[i].lower() in lexicon:\n",
        "                index_value = lexicon.index(word_tokens[i].lower())\n",
        "                features[index_value] += 1\n",
        "\n",
        "            features = list(features)\n",
        "            featureset.append([features, classification])\n",
        "    return featureset\n",
        "\n",
        "\n",
        "train_featureset = build_featureset(train_df,lexicon,classifications)\n",
        "test_featureset = build_featureset(test_df,lexicon,classifications)\n",
        "\n",
        "X_train = [features[0] for features in train_featureset]\n",
        "y_train = [features[1] for features in train_featureset]\n",
        "X_test = [features[0] for features in test_featureset]\n",
        "y_test = [features[1] for features in test_featureset]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPjwT-bZ49Ge",
        "colab_type": "text"
      },
      "source": [
        "## Build and run the model\n",
        "\n",
        "The Random Forest Classifier is built setting the number of trees in the forest to 100 and also calculating a class weight. The `effectiveness` classifications are not balanced which would create bias in the model so to counter this, weights are applied to each class, inversely proportional to the size of each class.\n",
        "\n",
        "The performance of the model is measured below in terms of Precision, Recall and F-values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4TwSYW74mwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "070d6d0c-c5bd-499b-d0f5-201c00c1ad47"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "#n_estimators defines the number of trees for the random forest\n",
        "regressor = RandomForestClassifier(n_estimators=100,random_state=1,verbose=3, class_weight='balanced')\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# The random forest model returns a matrix of floats, this needs to be transformed into integers\n",
        "# to define the clear cut classifications so that accuracy can be calculated.\n",
        "y_pred_float = regressor.predict(X_test)\n",
        "y_pred = []\n",
        "for y in y_pred_float:\n",
        "   y_pred.append(round(y))\n",
        "\n",
        "print(\"\\nRandom Forest Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(\"Random forest confusion matrix: \\n\")\n",
        "print(metrics.confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"\\nRandom forest precision score:\")\n",
        "print(metrics.precision_score(y_test,y_pred,average='weighted'))\n",
        "print(\"\\nRandom forest recall score:\")\n",
        "print(metrics.recall_score(y_test,y_pred,average='weighted'))\n",
        "print(\"\\nRandom forest f1 score:\")\n",
        "print(metrics.f1_score(y_test,y_pred,average='weighted'))\n",
        "\n",
        "print(\"classification report:\")\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.7s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 2 of 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    7.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 3 of 100\n",
            "building tree 4 of 100\n",
            "building tree 5 of 100\n",
            "building tree 6 of 100\n",
            "building tree 7 of 100\n",
            "building tree 8 of 100\n",
            "building tree 9 of 100\n",
            "building tree 10 of 100\n",
            "building tree 11 of 100\n",
            "building tree 12 of 100\n",
            "building tree 13 of 100\n",
            "building tree 14 of 100\n",
            "building tree 15 of 100\n",
            "building tree 16 of 100\n",
            "building tree 17 of 100\n",
            "building tree 18 of 100\n",
            "building tree 19 of 100\n",
            "building tree 20 of 100\n",
            "building tree 21 of 100\n",
            "building tree 22 of 100\n",
            "building tree 23 of 100\n",
            "building tree 24 of 100\n",
            "building tree 25 of 100\n",
            "building tree 26 of 100\n",
            "building tree 27 of 100\n",
            "building tree 28 of 100\n",
            "building tree 29 of 100\n",
            "building tree 30 of 100\n",
            "building tree 31 of 100\n",
            "building tree 32 of 100\n",
            "building tree 33 of 100\n",
            "building tree 34 of 100\n",
            "building tree 35 of 100\n",
            "building tree 36 of 100\n",
            "building tree 37 of 100\n",
            "building tree 38 of 100\n",
            "building tree 39 of 100\n",
            "building tree 40 of 100\n",
            "building tree 41 of 100\n",
            "building tree 42 of 100\n",
            "building tree 43 of 100\n",
            "building tree 44 of 100\n",
            "building tree 45 of 100\n",
            "building tree 46 of 100\n",
            "building tree 47 of 100\n",
            "building tree 48 of 100\n",
            "building tree 49 of 100\n",
            "building tree 50 of 100\n",
            "building tree 51 of 100\n",
            "building tree 52 of 100\n",
            "building tree 53 of 100\n",
            "building tree 54 of 100\n",
            "building tree 55 of 100\n",
            "building tree 56 of 100\n",
            "building tree 57 of 100\n",
            "building tree 58 of 100\n",
            "building tree 59 of 100\n",
            "building tree 60 of 100\n",
            "building tree 61 of 100\n",
            "building tree 62 of 100\n",
            "building tree 63 of 100\n",
            "building tree 64 of 100\n",
            "building tree 65 of 100\n",
            "building tree 66 of 100\n",
            "building tree 67 of 100\n",
            "building tree 68 of 100\n",
            "building tree 69 of 100\n",
            "building tree 70 of 100\n",
            "building tree 71 of 100\n",
            "building tree 72 of 100\n",
            "building tree 73 of 100\n",
            "building tree 74 of 100\n",
            "building tree 75 of 100\n",
            "building tree 76 of 100\n",
            "building tree 77 of 100\n",
            "building tree 78 of 100\n",
            "building tree 79 of 100\n",
            "building tree 80 of 100\n",
            "building tree 81 of 100\n",
            "building tree 82 of 100\n",
            "building tree 83 of 100\n",
            "building tree 84 of 100\n",
            "building tree 85 of 100\n",
            "building tree 86 of 100\n",
            "building tree 87 of 100\n",
            "building tree 88 of 100\n",
            "building tree 89 of 100\n",
            "building tree 90 of 100\n",
            "building tree 91 of 100\n",
            "building tree 92 of 100\n",
            "building tree 93 of 100\n",
            "building tree 94 of 100\n",
            "building tree 95 of 100\n",
            "building tree 96 of 100\n",
            "building tree 97 of 100\n",
            "building tree 98 of 100\n",
            "building tree 99 of 100\n",
            "building tree 100 of 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  5.6min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Random Forest Accuracy: 0.22197961122375193\n",
            "Random forest confusion matrix: \n",
            "\n",
            "[[ 5348  1985  1259  3381 15697]\n",
            " [ 1079   450   633   472  3039]\n",
            " [  780   459   542   354  3907]\n",
            " [ 2597  1313  1097  2316 13086]\n",
            " [ 1878   604   560   693  7000]]\n",
            "\n",
            "Random forest precision score:\n",
            "0.31628646011551453\n",
            "\n",
            "Random forest recall score:\n",
            "0.22197961122375193\n",
            "\n",
            "Random forest f1 score:\n",
            "0.2110800339738471\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.46      0.19      0.27     27670\n",
            "           2       0.09      0.08      0.09      5673\n",
            "           3       0.13      0.09      0.11      6042\n",
            "           4       0.32      0.11      0.17     20409\n",
            "           5       0.16      0.65      0.26     10735\n",
            "\n",
            "    accuracy                           0.22     70529\n",
            "   macro avg       0.23      0.23      0.18     70529\n",
            "weighted avg       0.32      0.22      0.21     70529\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}