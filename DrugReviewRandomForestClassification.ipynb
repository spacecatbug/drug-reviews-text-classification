{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DrugReviewRandomForestClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1L0vAsRRFuh_Nc1yURJ5a7aMB3xYHiObm",
      "authorship_tag": "ABX9TyPozb2FxvQaIobmSq8L4c7S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9w1r7sT_6AV",
        "colab_type": "text"
      },
      "source": [
        "## Data Import\n",
        "\n",
        "Python libraries required: numpy, pandas and tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JOaQ9yV_31B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "36a2af2d-e84c-4c7f-cebd-c914524472a3"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "!pip install tensorflow-hub\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#print(\"Version: \", tf.__version__)\n",
        "#print(\"Eager mode: \", tf.executing_eagerly())\n",
        "#print(\"Hub version: \", hub.__version__)\n",
        "#print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (49.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAACYqeYAHJ3",
        "colab_type": "text"
      },
      "source": [
        "Optional: If working in Google Colab, use drive.mount() so that you can import files from Google Drive into your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SOFbetdALK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "06f3b83a-7332-48b6-8e24-4b00f1eb171c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qma0jQoRCRsy",
        "colab_type": "text"
      },
      "source": [
        "## Initial data inspection\n",
        "Load in test and training data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCEyP7h2CQ_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = '/content/drive/My Drive/drugLib_raw/drugLibTrain_raw.tsv'\n",
        "test_file = '/content/drive/My Drive/drugLib_raw/drugLibTest_raw.tsv'\n",
        "\n",
        "train_df = pd.read_csv(train_file,sep='\\t')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt4XdBDJGbUt",
        "colab_type": "text"
      },
      "source": [
        "This dataset contains 8 columns column describing drug information, patient condition, the resultant treatment, effectiveness and patient reviews. \n",
        "\n",
        "In this text classification exercise the aim is to predict the `effectiveness` from this dataset. \n",
        "\n",
        "There are 3 columns of descriptive patient review data: `benefitsReview`, `sideEffectsReview` and `commentsReview`. \n",
        "\n",
        "The data in these columns will be used for the text classification to predict the `effectiveness`.\n",
        "\n",
        "This is a publically available dataset that can be found here along with a more comprehensive description of the data:\n",
        "https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Druglib.com%29\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWeY9FztHKBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "97641723-ca66-44a2-910f-f42158e8e25c"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>effectiveness</th>\n",
              "      <th>sideEffects</th>\n",
              "      <th>condition</th>\n",
              "      <th>benefitsReview</th>\n",
              "      <th>sideEffectsReview</th>\n",
              "      <th>commentsReview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2202</td>\n",
              "      <td>enalapril</td>\n",
              "      <td>4</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>management of congestive heart failure</td>\n",
              "      <td>slowed the progression of left ventricular dys...</td>\n",
              "      <td>cough, hypotension , proteinuria, impotence , ...</td>\n",
              "      <td>monitor blood pressure , weight and asses for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3117</td>\n",
              "      <td>ortho-tri-cyclen</td>\n",
              "      <td>1</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>birth prevention</td>\n",
              "      <td>Although this type of birth control has more c...</td>\n",
              "      <td>Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...</td>\n",
              "      <td>I Hate This Birth Control, I Would Not Suggest...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1146</td>\n",
              "      <td>ponstel</td>\n",
              "      <td>10</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>No Side Effects</td>\n",
              "      <td>menstrual cramps</td>\n",
              "      <td>I was used to having cramps so badly that they...</td>\n",
              "      <td>Heavier bleeding and clotting than normal.</td>\n",
              "      <td>I took 2 pills at the onset of my menstrual cr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3947</td>\n",
              "      <td>prilosec</td>\n",
              "      <td>3</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>acid reflux</td>\n",
              "      <td>The acid reflux went away for a few months aft...</td>\n",
              "      <td>Constipation, dry mouth and some mild dizzines...</td>\n",
              "      <td>I was given Prilosec prescription at a dose of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1951</td>\n",
              "      <td>lyrica</td>\n",
              "      <td>2</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>fibromyalgia</td>\n",
              "      <td>I think that the Lyrica was starting to help w...</td>\n",
              "      <td>I felt extremely drugged and dopey.  Could not...</td>\n",
              "      <td>See above</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                     commentsReview\n",
              "0        2202  ...  monitor blood pressure , weight and asses for ...\n",
              "1        3117  ...  I Hate This Birth Control, I Would Not Suggest...\n",
              "2        1146  ...  I took 2 pills at the onset of my menstrual cr...\n",
              "3        3947  ...  I was given Prilosec prescription at a dose of...\n",
              "4        1951  ...                                          See above\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai4AlIbmCxH3",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "The data needs to be pre-processed before inputing it into a model.\n",
        "A model will perform better if the input data consist of features that have a significant impact on what you are trying to predict and the amount of noise is minimized (data that is deemed insignificant). Several techniques are used here in this analysis exercise:\n",
        "\n",
        "1.   Data cleansing\n",
        "2.   Lemmatization\n",
        "3.   Removal of stop words\n",
        "4.   Using term frequency-inverse document frequency (TF-IDF)\n",
        "\n",
        "### 1. Data cleansing\n",
        "\n",
        "The train and test data is currently in tab delimited format and will be converted into Pandas Dataframes. \n",
        "\n",
        "An additional column `combinedReview` has been added which contains all the review data from the 3 columns, concatenated.\n",
        "\n",
        "Another additional column `label` has been included in these Dataframes that assigns classification labels `effectiveness` as integer values so that they can be read later by the model.\n",
        "\n",
        "The `combinedReview` data will be cleaned to remove any special (invalid) characters, multiple spaces, numbers, escape characters and any words that are just 1 character long. The review text will also be case-folded (converted all to lower case) so that words that are spelled the same will be grouped together regardless of their upper/lower casing.\n",
        "\n",
        "### 2. Lemmatization\n",
        "\n",
        "Lemmatization takes words and reduces them down to its base form i.e. its lemma. This helps to group together words that are similar and can be considered equivalent for the purposes of input features for a model. For example, the words `bird` and `birds` have the same lemma, `bird`, so they will be grouped together.\n",
        "\n",
        "### 3. Removal of stop words\n",
        "Stop words, or function words, are words that typically do not provide a lot of information for text mining and are not useful to include in the model. Stop words are the most commonly used words, e.g. this, the, a, of. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E9mobvQCwxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "770a62d9-722f-4a52-f92e-682aa4a21dcd"
      },
      "source": [
        "labels_dict = {}\n",
        "for count,label in enumerate(train_df[\"effectiveness\"].unique()):\n",
        "  labels_dict[label] = count+1\n",
        "\n",
        "def tsv2df(filename):\n",
        "  df = pd.read_csv(filename,sep='\\t')\n",
        "  df[\"combinedReview\"] = np.nan\n",
        "  df[\"label\"] = np.nan\n",
        "  for row in df.itertuples():\n",
        "    drug_review = \"\"\n",
        "    benefitsReview = df.loc[row.Index,[\"benefitsReview\"]].values[0]\n",
        "    sideEffectsReview = df.loc[row.Index,[\"sideEffectsReview\"]].values[0]\n",
        "    commentsReview = df.loc[row.Index,[\"commentsReview\"]].values[0]\n",
        "    # concatenate review data from all 3 columns into a new column\n",
        "    if not pd.isnull(benefitsReview):\n",
        "      drug_review += ' '.join(clean_review_text(benefitsReview))\n",
        "    if not pd.isnull(sideEffectsReview):\n",
        "      drug_review += ' ' + ' '.join(clean_review_text(sideEffectsReview))\n",
        "    if not pd.isnull(commentsReview):\n",
        "      drug_review += ' ' + ' '.join(clean_review_text(commentsReview))\n",
        "    if drug_review.strip() == \"\":\n",
        "      continue\n",
        "    df.loc[row.Index,[\"combinedReview\"]] = drug_review\n",
        "    # Use integers to define classification labels\n",
        "    df.loc[row.Index,[\"label\"]] = labels_dict[train_df.loc[row.Index,[\"effectiveness\"]].values[0]]\n",
        "  return df\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Function used to clean the test and training drug review data and returns a tokenized list of the words in their lemma form(root form)\n",
        "def clean_review_text(drug_review):\n",
        "    # Remove special characters\n",
        "    drug_review = re.sub('\\W', ' ', drug_review)\n",
        "    # Remove underscores\n",
        "    drug_review = re.sub('_', '', drug_review)\n",
        "    # Remove single characters\n",
        "    drug_review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', drug_review)\n",
        "    # Remove all numbers\n",
        "    drug_review = re.sub(\"\\d+\", \"\", drug_review)\n",
        "    # Remove single characters from the start\n",
        "    drug_review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', drug_review)\n",
        "    # Substituting multiple spaces with single space\n",
        "    drug_review = re.sub(r'\\s+', ' ', drug_review, flags=re.I)\n",
        "    # Removing prefixed 'b'\n",
        "    drug_review = re.sub(r'^b\\s+', '', drug_review)\n",
        "    # Remove any newline escape characters\n",
        "    drug_review = re.sub(\"\\n\", \"\", drug_review)\n",
        "    # Converting to Lowercase\n",
        "    drug_review = drug_review.lower()\n",
        "    # Lemmatization\n",
        "    drug_review = drug_review.split()\n",
        "    # Replace each word in each text file with its lemma form (root form)\n",
        "    drug_review = [WordNetLemmatizer().lemmatize(word) for word in drug_review]\n",
        "    # Remove stop words\n",
        "    stops = stopwords.words('english')\n",
        "    clean_drug_review = []\n",
        "\n",
        "    postags = nltk.pos_tag(drug_review)\n",
        "    for tag in postags:\n",
        "        word = tag[0]\n",
        "        pos = tag[1]\n",
        "        if pos[:2] == \"NN\" or pos[0] == \"V\" or pos[:2] == \"RB\" or pos[0] == \"J\":\n",
        "            if not word in stops and len(word) > 2:\n",
        "                clean_drug_review.append(word)\n",
        "    return clean_drug_review\n",
        "\n",
        "\n",
        "train_df = tsv2df(train_file)\n",
        "test_df = tsv2df(test_file)\n",
        "\n",
        "# Remove any duplicate lines in the train dataset and also any rows with no patient review data (no input for the model)\n",
        "train_df.drop_duplicates()\n",
        "train_df.dropna(subset=[\"combinedReview\"],inplace=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUdBLX-Bxrhs",
        "colab_type": "text"
      },
      "source": [
        "Viewing the updated, cleaned Dataframe for the training dataset which includes the `combinedReview` column and the classification labels from the `effectiveness` mapped to integers in the `label` column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1wX-o6qxpA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "62c53c38-4fc8-4d54-b7ed-3cc811a7694b"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>effectiveness</th>\n",
              "      <th>sideEffects</th>\n",
              "      <th>condition</th>\n",
              "      <th>benefitsReview</th>\n",
              "      <th>sideEffectsReview</th>\n",
              "      <th>commentsReview</th>\n",
              "      <th>combinedReview</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2202</td>\n",
              "      <td>enalapril</td>\n",
              "      <td>4</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>management of congestive heart failure</td>\n",
              "      <td>slowed the progression of left ventricular dys...</td>\n",
              "      <td>cough, hypotension , proteinuria, impotence , ...</td>\n",
              "      <td>monitor blood pressure , weight and asses for ...</td>\n",
              "      <td>slowed progression left ventricular dysfunctio...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3117</td>\n",
              "      <td>ortho-tri-cyclen</td>\n",
              "      <td>1</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>birth prevention</td>\n",
              "      <td>Although this type of birth control has more c...</td>\n",
              "      <td>Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...</td>\n",
              "      <td>I Hate This Birth Control, I Would Not Suggest...</td>\n",
              "      <td>type birth control con help cramp also effecti...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1146</td>\n",
              "      <td>ponstel</td>\n",
              "      <td>10</td>\n",
              "      <td>Highly Effective</td>\n",
              "      <td>No Side Effects</td>\n",
              "      <td>menstrual cramps</td>\n",
              "      <td>I was used to having cramps so badly that they...</td>\n",
              "      <td>Heavier bleeding and clotting than normal.</td>\n",
              "      <td>I took 2 pills at the onset of my menstrual cr...</td>\n",
              "      <td>used cramp badly leave balled bed least day po...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3947</td>\n",
              "      <td>prilosec</td>\n",
              "      <td>3</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Mild Side Effects</td>\n",
              "      <td>acid reflux</td>\n",
              "      <td>The acid reflux went away for a few months aft...</td>\n",
              "      <td>Constipation, dry mouth and some mild dizzines...</td>\n",
              "      <td>I was given Prilosec prescription at a dose of...</td>\n",
              "      <td>acid reflux went away month day drug heartburn...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1951</td>\n",
              "      <td>lyrica</td>\n",
              "      <td>2</td>\n",
              "      <td>Marginally Effective</td>\n",
              "      <td>Severe Side Effects</td>\n",
              "      <td>fibromyalgia</td>\n",
              "      <td>I think that the Lyrica was starting to help w...</td>\n",
              "      <td>I felt extremely drugged and dopey.  Could not...</td>\n",
              "      <td>See above</td>\n",
              "      <td>think lyrica starting help pain side effect se...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... label\n",
              "0        2202  ...   1.0\n",
              "1        3117  ...   1.0\n",
              "2        1146  ...   1.0\n",
              "3        3947  ...   2.0\n",
              "4        1951  ...   2.0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA_-v8LVQo97",
        "colab_type": "text"
      },
      "source": [
        "### 4. Using term frequency-inverse document frequency (TF-IDF)\n",
        "The reviews contain a very large range of words and it would take a lot of computational power to include every single unique word when training a model. We can use term frequency-inverse document frequency (TF-IDF) which is a statistic that measures how important words are to a corpus (the set of all reviews). TF-IDF depends on how many times a word appears in a single document and also how many different documents that word appears in.\n",
        "\n",
        "Higher TF-IDF values are given to words that appear  frequently within a single document *AND* if that word appears in a smaller number of documents. This means that if these words are seen in a document, it will be easier to predict what classification that document belongs to, based on that word. Using intuition, a word like `the` would have a low TF-IDF because it would probably appear in all documents. A word like `headache` may have a high TF-IDF because it seems like a word that would appear in a smaller number of reviews, and intuitively it would seem that this word highly suggests that the drug review is negative, for example. A word that is completely unique and only appears once in all the reviews would have a low TF-IDF because it appears infrequently in a single document (an example of this could be a misspelled word, like `headahce`.\n",
        "\n",
        "We will also allow for up to groups of 2 words and 3 words (called bi-grams and tri-grams) to be included for this analysis. In this example, `side effect` is a bi-gram and results in a high TF-IDF score of 54.122304."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHByVeesQor9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "c1268f85-e0dc-4635-f0e2-454934b6affd"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(ngram_range = (1,3),\n",
        "                     sublinear_tf = True,\n",
        "                     max_features = 40000)\n",
        "train_tv = tv.fit_transform(train_df['combinedReview'])\n",
        "vocab = tv.get_feature_names()\n",
        "dist = np.sum(train_tv, axis=0)\n",
        "vocab_tfidf_df = pd.DataFrame(dist,columns = vocab)\n",
        "sorted_tfidf_df = vocab_tfidf_df.sort_values(vocab_tfidf_df.first_valid_index(),axis=1,ascending=False)\n",
        "print(\"TF-IDF scores for terms sorted by the highest score from left to right\")\n",
        "sorted_tfidf_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF-IDF scores for terms sorted by the highest score from left to right\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>effect</th>\n",
              "      <th>side</th>\n",
              "      <th>take</th>\n",
              "      <th>side effect</th>\n",
              "      <th>taking</th>\n",
              "      <th>drug</th>\n",
              "      <th>time</th>\n",
              "      <th>pain</th>\n",
              "      <th>week</th>\n",
              "      <th>medication</th>\n",
              "      <th>treatment</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>also</th>\n",
              "      <th>took</th>\n",
              "      <th>pill</th>\n",
              "      <th>skin</th>\n",
              "      <th>none</th>\n",
              "      <th>first</th>\n",
              "      <th>night</th>\n",
              "      <th>get</th>\n",
              "      <th>sleep</th>\n",
              "      <th>daily</th>\n",
              "      <th>feel</th>\n",
              "      <th>hour</th>\n",
              "      <th>started</th>\n",
              "      <th>doctor</th>\n",
              "      <th>symptom</th>\n",
              "      <th>felt</th>\n",
              "      <th>dose</th>\n",
              "      <th>work</th>\n",
              "      <th>depression</th>\n",
              "      <th>taken</th>\n",
              "      <th>better</th>\n",
              "      <th>much</th>\n",
              "      <th>severe</th>\n",
              "      <th>prescribed</th>\n",
              "      <th>still</th>\n",
              "      <th>morning</th>\n",
              "      <th>...</th>\n",
              "      <th>really never</th>\n",
              "      <th>rage time minor</th>\n",
              "      <th>rage medication</th>\n",
              "      <th>really mad thing</th>\n",
              "      <th>quit warfarin several</th>\n",
              "      <th>much blood</th>\n",
              "      <th>really doctor</th>\n",
              "      <th>realized headache able</th>\n",
              "      <th>realized headache</th>\n",
              "      <th>really doctor tell</th>\n",
              "      <th>really feel depressed</th>\n",
              "      <th>quit scenario</th>\n",
              "      <th>quit scenario repeated</th>\n",
              "      <th>much blood give</th>\n",
              "      <th>quit warfarin started</th>\n",
              "      <th>substance resulting</th>\n",
              "      <th>substance resulting small</th>\n",
              "      <th>wet substance resulting</th>\n",
              "      <th>reason discontinued use</th>\n",
              "      <th>quit taking altogether</th>\n",
              "      <th>reason forgot</th>\n",
              "      <th>much fun reason</th>\n",
              "      <th>quite sure thing</th>\n",
              "      <th>much done day</th>\n",
              "      <th>nearly healthy</th>\n",
              "      <th>nearly healthy mentally</th>\n",
              "      <th>realize severe</th>\n",
              "      <th>reason stopped taking</th>\n",
              "      <th>reason stopped</th>\n",
              "      <th>realize pulling</th>\n",
              "      <th>realize pulling even</th>\n",
              "      <th>much coffee made</th>\n",
              "      <th>realize severe anxiety</th>\n",
              "      <th>really weird way</th>\n",
              "      <th>much coffee</th>\n",
              "      <th>really well taking</th>\n",
              "      <th>reason forgot take</th>\n",
              "      <th>reason discontinued</th>\n",
              "      <th>mental focus submitting</th>\n",
              "      <th>mental focus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71.974762</td>\n",
              "      <td>62.70226</td>\n",
              "      <td>57.838698</td>\n",
              "      <td>57.114266</td>\n",
              "      <td>55.32922</td>\n",
              "      <td>52.28423</td>\n",
              "      <td>50.247639</td>\n",
              "      <td>50.216333</td>\n",
              "      <td>49.028411</td>\n",
              "      <td>46.310573</td>\n",
              "      <td>45.557485</td>\n",
              "      <td>41.870218</td>\n",
              "      <td>40.678125</td>\n",
              "      <td>40.411878</td>\n",
              "      <td>39.897685</td>\n",
              "      <td>38.878173</td>\n",
              "      <td>38.628507</td>\n",
              "      <td>37.770989</td>\n",
              "      <td>35.598845</td>\n",
              "      <td>35.582818</td>\n",
              "      <td>34.347453</td>\n",
              "      <td>33.321561</td>\n",
              "      <td>32.97327</td>\n",
              "      <td>32.601669</td>\n",
              "      <td>32.003928</td>\n",
              "      <td>31.67047</td>\n",
              "      <td>31.517658</td>\n",
              "      <td>31.439981</td>\n",
              "      <td>31.225123</td>\n",
              "      <td>30.099893</td>\n",
              "      <td>29.675387</td>\n",
              "      <td>29.598504</td>\n",
              "      <td>29.485145</td>\n",
              "      <td>29.385297</td>\n",
              "      <td>28.892024</td>\n",
              "      <td>28.54432</td>\n",
              "      <td>28.525362</td>\n",
              "      <td>28.294834</td>\n",
              "      <td>27.998296</td>\n",
              "      <td>27.925632</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061973</td>\n",
              "      <td>0.061973</td>\n",
              "      <td>0.061973</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.061058</td>\n",
              "      <td>0.059557</td>\n",
              "      <td>0.059557</td>\n",
              "      <td>0.059557</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.059015</td>\n",
              "      <td>0.055737</td>\n",
              "      <td>0.055737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 40000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         day    effect  ...  mental focus submitting  mental focus\n",
              "0  71.974762  62.70226  ...                 0.055737      0.055737\n",
              "\n",
              "[1 rows x 40000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7HF7kPRsIRX",
        "colab_type": "text"
      },
      "source": [
        "Create the lexicon by calculating Term Frequency - Infrequent Document Frequency (TF-IDF). We will save the terms with the top 600 TF-IDF scores as the vocabulary chosen as input features for this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA0jxm6qsB00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "67eaa8d5-57bd-4bff-c82f-48ce4015a92b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def tfidf_lexicon(train_df,size=400):\n",
        "    tv = TfidfVectorizer(ngram_range = (1,3),\n",
        "                         sublinear_tf = True,\n",
        "                         max_features = 40000)\n",
        "    train_tv = tv.fit_transform(train_df['combinedReview'])\n",
        "    vocab = tv.get_feature_names()\n",
        "    dist = np.sum(train_tv, axis=0)\n",
        "    vocab_tfidf_df = pd.DataFrame(dist,columns = vocab)\n",
        "    sorted_tfidf_df = vocab_tfidf_df.sort_values(vocab_tfidf_df.first_valid_index(),axis=1,ascending=False)\n",
        "    i = 0\n",
        "    target_vocab = []\n",
        "    for (word, score) in sorted_tfidf_df.iteritems():\n",
        "        target_vocab.append((word, score.values[0]))\n",
        "        i += 1\n",
        "        if i == size:\n",
        "            break\n",
        "    return target_vocab\n",
        "\n",
        "size = 600\n",
        "lexicon_tfidf = tfidf_lexicon(train_df, size)\n",
        "lexicon = [l[0] for l in lexicon_tfidf]\n",
        "\n",
        "print(\"Target vocabulary to use for text classification are the words with the top \"+ str(size) + \" TF-IDF scores:\")\n",
        "print(', '.join(term for term in lexicon))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target vocabulary to use for text classification are the words with the top 600 TF-IDF scores:\n",
            "day, effect, side, take, side effect, taking, drug, time, pain, week, medication, treatment, year, month, also, took, pill, skin, none, first, night, get, sleep, daily, feel, hour, started, doctor, symptom, felt, dose, work, depression, taken, better, much, severe, prescribed, still, morning, back, use, able, help, feeling, headache, anxiety, acne, problem, benefit, well, weight, blood, stopped, dry, tablet, helped, even, used, increased, effective, went, reduced, life, stomach, dosage, mild, loss, made, infection, experienced, really, due, period, good, long, face, however, never, little, control, nausea, result, mood, level, noticed, normal, patient, tried, bad, mouth, pressure, got, stop, needed, medicine, thing, body, using, twice, worked, make, became, eye, hair, experience, high, need, almost, see, away, think, caused, migraine, found, completely, longer, several, began, know, food, going, tired, sometimes, blood pressure, gain, bed, thought, increase, take pill, decreased, relief, seemed, cleared, cream, many, area, slight, lower, energy, heart, getting, minute, improved, next, told, put, great, low, change, weight gain, lot, sure, last, taking medication, product, insomnia, water, keep, med, condition, attack, pill day, worse, leg, given, far, doe, muscle, eat, try, later, amount, usually, cause, reduction, start, dry mouth, appetite, gone, allergy, redness, improvement, people, immediately, decrease, time day, prescription, applied, couple, fatigue, nothing, function, extreme, reaction, continue, taking drug, evening, starting, antibiotic, extremely, twice day, acid, continued, day day, joint, reduce, always, way, test, come, hot, enough, asleep, therapy, overall, right, say, drowsiness, dizziness, clear, sleeping, small, cold, quickly, something, first week, surgery, gave, sex, lost, seem, anti, sinus, anything, drive, apply, stopped taking, started taking, actually, bit, issue, least, seems, want, full, term, depressed, ever, hard, said, half, swelling, notice, new, affect, believe, bedtime, beginning, different, working, panic, upset, second, stay, nose, easy, hand, red, often, short, point, sore, dryness, constipation, changed, sun, rash, find, ago, course, irritation, sexual, moderate, exercise, wake, possible, finally, activity, take tablet, left, head, reflux, flash, dos, initially, stress, throat, came, difference, cholesterol, hot flash, especially, significant, quite, difficult, treat, diet, itching, lowered, become, developed, eating, treated, memory, neck, disappeared, negative, went away, regular, slightly, chronic, significantly, prior, libido, best, focus, positive, thyroid, fall, lexapro, physician, free, diagnosed, much better, eventually, pound, done, trying, noticeable, difficulty, care, terrible, ability, combination, discomfort, end, generic, suffered, injection, case, constant, swing, subsided, birth, major, give, horrible, soon, quit, fine, kept, past, patch, unable, mood swing, look, returned, physical, panic attack, family, episode, slowly, take medication, part, treatment benefit, pretty, light, asthma, relieved, bone, lip, birth control, nasal, spot, retin, painful, etc, ache, peeling, experiencing, hair loss, allowed, smoking, recommended, initial, return, happy, occasional, sensitive, eliminated, taste, break, weight loss, prozac, general, gradually, entire, recommend, chest, diarrhea, sweat, sensitivity, dry skin, wellbutrin, instead, decided, everyday, old, sex drive, taking pill, lasted, next day, stop taking, counter, somewhat, situation, took pill, remember, rid, form, first day, burning, affected, year ago, application, following, easily, foot, cramp, prevent, brain, worst, drink, breast, take day, trouble, complete, highly, job, day week, tell, tiredness, vaginal, probably, cough, inflammation, meal, associated, withdrawal, switched, dream, arm, system, yeast, wanted, bleeding, doctor prescribed, others, pregnancy, hormone, thinking, acid reflux, read, disorder, basis, long term, else, line, whole, cycle, sleepy, gained, lack, allergic, definitely, twice daily, size, seizure, fast, rest, onset, mind, fall asleep, reason, greatly, effexor, rather, age, already, supposed, strong, side affect, drop, approximately, arthritis, child, type, maybe, normally, disease, lessened, lose, blood test, early, sleep night, addition, capsule, adverse, cry, related, ear, controlled, rate, person, including, tablet day, vomiting, anymore, itchy, saw, empty, awake, number, wrinkle, detail, liver, waking, hospital, pregnant, yeast infection, first time, walk, topical, anyone, total, knee, dont, take drug, yet, craving, sinus infection, gel, cannot, non, higher, order, suffer, emotional, night sweat, minimal, alot, healthy, self, effectively, looking, aware, included, stopping, everything, minor, oral, breath, damage, wait, breakfast, hive, easier, heartburn, sensation, noted, first month, nightly, breakout, occasionally, natural, sugar, anxious, severity, school, heavy, dermatologist, avoid, immediate, frequent, home, deal, empty stomach, reducing, unfortunately\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUEo_uw-zo7K",
        "colab_type": "text"
      },
      "source": [
        "Building the classifications which maps distinct classes from the dataset to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvajqCiczzcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4437e576-7ec6-4fa4-882a-d038a8b5c99c"
      },
      "source": [
        "def build_classifications(df):\n",
        "    classification_names = list(df.effectiveness.unique())\n",
        "    classifications = {}\n",
        "    id = 1\n",
        "    for classification in classification_names:\n",
        "        classifications[classification] = id\n",
        "        id += 1\n",
        "    return classifications\n",
        "\n",
        "classifications = build_classifications(train_df)\n",
        "print(classifications)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Highly Effective': 1, 'Marginally Effective': 2, 'Ineffective': 3, 'Considerably Effective': 4, 'Moderately Effective': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lInzKNaxz_eu",
        "colab_type": "text"
      },
      "source": [
        "Building the final feature sets to use for the Random Forest model. The final feature set is converted into a vector representing the frequency of words from the targeted vocabulary that are present in each review. Since the vocabulary is 600 words long, the vector representation for each review is also 600 numbers long. Taking the target vocabulary that is printed in the code block above, the first 3 vocab words are day, effect and side.\n",
        "If the review text is `I experience this side effect on a day to day basis` then the vector representation would be [2,1,2] because `day` appears twice and `side` and `effect` appear once only.\n",
        "\n",
        "*   `X_train` contains the vector representation of the reviews from the training dataset. The model will use this dataset to train.\n",
        "*   `y_train` represents the classification for that review. Each review is represented by one number. For example, if the `effectiveness` of a drug review is `Highly Effective`, then this is represented in the `y_train` dataset as `1`. There are 3097 reviews in the final training dataset, so `y_train` would simply be a list of 3097 numbers, ranging from 1 to 5 for the different classes.\n",
        "The model will use this dataset to train.\n",
        "\n",
        "*   `X_test` contains the vector representation of the reviews from the test dataset. After the model is trained, it will use this as input features to try and predict the effectiveness.\n",
        "*   `y_test` contains the actual effectiveness for the reviews in the test dataset. After the model has predicted the classifications, its guesses will be compared against this dataset to measure its accuracy, precision and recall later.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzGWVsNa0EPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "73dfbcfc-eb19-4496-f53e-3c707025dc80"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def build_featureset(df,lexicon,classifications):\n",
        "    featureset = []\n",
        "\n",
        "    # N-gram terms from the lexicon kept as a separate list\n",
        "    ngrams = [ngram for ngram in lexicon if (len(ngram.split()) > 1)]\n",
        "\n",
        "    for row in df.itertuples():\n",
        "        combined_drug_review = df.loc[row.Index, \"combinedReview\"]\n",
        "        # If there is no review available for a row, no need to process it, continue on processing the next line\n",
        "        if pd.isnull(combined_drug_review):\n",
        "            continue\n",
        "        word_tokens = word_tokenize(combined_drug_review.lower())\n",
        "        features = np.zeros(len(lexicon))\n",
        "        classification = classifications[row.effectiveness]\n",
        "        for i in range(len(word_tokens)):\n",
        "            # Check match to n-grams first\n",
        "            for ngram in ngrams:\n",
        "                if ngram in combined_drug_review:\n",
        "                    index_value = lexicon.index(ngram.lower())\n",
        "                    features[index_value] += 1\n",
        "\n",
        "            # Matching one word terms\n",
        "            if word_tokens[i].lower() in lexicon:\n",
        "                index_value = lexicon.index(word_tokens[i].lower())\n",
        "                features[index_value] += 1\n",
        "\n",
        "            features = list(features)\n",
        "            featureset.append([features, classification])\n",
        "    return featureset\n",
        "\n",
        "\n",
        "train_featureset = build_featureset(train_df,lexicon,classifications)\n",
        "test_featureset = build_featureset(test_df,lexicon,classifications)\n",
        "\n",
        "X_train = [features[0] for features in train_featureset]\n",
        "y_train = [features[1] for features in train_featureset]\n",
        "X_test = [features[0] for features in test_featureset]\n",
        "y_test = [features[1] for features in test_featureset]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPjwT-bZ49Ge",
        "colab_type": "text"
      },
      "source": [
        "## Building the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4TwSYW74mwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "159ad737-5262-4009-e06d-88621a13a90a"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "\n",
        "#n_estimators defines the number of trees for the random forest\n",
        "regressor = RandomForestRegressor(n_estimators=1,random_state=0,verbose=2)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# # The random forest model returns a matrix of floats, this needs to be transformed into integers\n",
        "# # to define the clear cut classifications so that accuracy can be calculated.\n",
        "# y_pred_float = regressor.predict(X_test)\n",
        "# y_pred = []\n",
        "# for y in y_pred_float:\n",
        "#    y_pred.append(round(y))\n",
        "\n",
        "\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
        "results = pd.DataFrame(columns=['training_score', 'test_score'])\n",
        "fprs, tprs, scores = [], [], []\n",
        "    \n",
        "for (train, test), i in zip(cv.split(X, y), range(5)):\n",
        "    regressor.fit(X.iloc[train], y.iloc[train])\n",
        "    _, _, auc_score_train = compute_roc_auc(train)\n",
        "    fpr, tpr, auc_score = compute_roc_auc(test)\n",
        "    scores.append((auc_score_train, auc_score))\n",
        "    fprs.append(fpr)\n",
        "    tprs.append(tpr)\n",
        "\n",
        "\n",
        "print(\"\\nRandom Forest Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(\"Random forest confusion matrix: \\n\")\n",
        "print(metrics.confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"\\n Random forest precision: \" )\n",
        "\n",
        "print(\"\\nRandom forest precision score:\")\n",
        "print(metrics.precision_score(y_test,y_pred,average='weighted'))\n",
        "print(\"\\nRandom forest recall score:\")\n",
        "print(metrics.recall_score(y_test,y_pred,average='weighted'))\n",
        "print(\"\\nRandom forest f1 score:\")\n",
        "print(metrics.f1_score(y_test,y_pred,average='weighted'))\n",
        "\n",
        "print(\"classification report:\")\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-682853524318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mfprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_score_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_roc_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKiLnwJhkB4m",
        "colab_type": "text"
      },
      "source": [
        "Convert train and test dataframes into Tensorflow datasets. Split the full train dataset into validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSalX_3gkHrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "5bddee3e-1211-42ea-a33e-4931e08b0487"
      },
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('label')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds\n",
        "\n",
        "full_train_tfds = df_to_dataset(train_df)\n",
        "test_tfds = df_to_dataset(test_df)\n",
        "\n",
        "full_train_tfds.shuffle(32)\n",
        "def is_val(x, y):\n",
        "    return x % 4 == 0\n",
        "\n",
        "def is_train(x, y):\n",
        "    return not is_val(x, y)\n",
        "\n",
        "recover = lambda x,y: y\n",
        "\n",
        "val_tfds = full_train_tfds.enumerate() \\\n",
        "                    .filter(is_val) \\\n",
        "                    .map(recover)\n",
        "\n",
        "train_tfds = full_train_tfds.enumerate() \\\n",
        "                    .filter(is_train) \\\n",
        "                    .map(recover)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    465\u001b[0m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0;32m--> 466\u001b[0;31m                   (element, type(element).__name__))\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for 0       monitor blood pressure , weight and asses for ...\n1       I Hate This Birth Control, I Would Not Suggest...\n2       I took 2 pills at the onset of my menstrual cr...\n3       I was given Prilosec prescription at a dose of...\n4                                               See above\n                              ...                        \n3102    I took adderall once as a child, and it made m...\n3103    I was on Zoloft for about 2 years total. I am ...\n3104                                                  ---\n3105    Started at 2 doses of 300 mg a day and worked ...\n3106             I take Micardis in pill form once daily.\nName: commentsReview, Length: 3107, dtype: object with type Series",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-1c6615429a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfull_train_tfds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtest_tfds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-1c6615429a3a>\u001b[0m in \u001b[0;36mdf_to_dataset\u001b[0;34m(dataframe, shuffle, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \"\"\"\n\u001b[0;32m--> 682\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2999\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         normalized_components.append(\n\u001b[0;32m---> 98\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                          as_ref=False):\n\u001b[1;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m    263\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 264\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez6vJCYclOem",
        "colab_type": "text"
      },
      "source": [
        "Optional: Run the lines of code below you would like to view examples of the features and labels created in the Tensorflow Dataset for one batch of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjdC89Ymk1e3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "4666be8d-0189-4b52-caf5-580a8961a96e"
      },
      "source": [
        "for feature_batch, label_batch in train_tfds.take(1):\n",
        "  print('Every feature:', list(feature_batch.keys()))\n",
        "  print('A batch of combinedReviews:', feature_batch['combinedReview'])\n",
        "  print('A batch of targets:', label_batch )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bbee07beb9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_tfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Every feature:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A batch of combinedReviews:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'combinedReview'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A batch of targets:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_tfds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQw9JUedmfnV",
        "colab_type": "text"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Creating a Keras layer using a pre-trained model from TensorFlow Hub to convert the `combined reviews` into embeddings. The embedding converts each `combined review` into a 20 dimension array (despite the length and contents of the review). An example of a sentence embedding is printed here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5wE_3UiJtc4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "5f8e7deb-6f25-4d34-da67-5bdf5b662996"
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "a = hub_layer(feature_batch['combinedReview'])\n",
        "# Prints one 20 dimensional embedding array \n",
        "a[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd9274ff730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd9274ff730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd9270c97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd9270c97b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd9275e3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fd9275e3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
              "array([ 3.9738226, -2.4324079,  1.7600758,  1.7758725, -4.506548 ,\n",
              "       -4.833144 , -3.5563126,  4.78504  ,  4.196502 , -0.4463017,\n",
              "       -3.0088549,  4.280519 ,  0.5396668,  0.6327668, -6.863371 ,\n",
              "        2.5921571,  5.555472 , -3.1780574, -3.8864355, -2.6467786],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zK_HxF4bYaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "a08f419b-8caf-414c-eb0e-305724b7eeb7"
      },
      "source": [
        "label_batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=float64, numpy=\n",
              "array([3., 2., 4., 1., 3., 3., 2., 1., 5., 5., 1., 1., 2., 5., 3., 3., 1.,\n",
              "       3., 3., 2., 5., 3., 3., 5., 5., 1., 5., 1., 3., 2., 1., 3.])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKuRmxc4nYon",
        "colab_type": "text"
      },
      "source": [
        "Build the model using a pre-trained model from Tensorflow. This model contains text embeddings which is trained on English Google News (130GB corpus). \n",
        "\n",
        "\n",
        "https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\n",
        "\n",
        "After the model is built a summary is printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otxFP8XPm876",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f3fb0992-33d7-47b1-e99c-0657c38f38f1"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_19 (KerasLayer)  (None, 20)                400020    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 16)                336       \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 400,373\n",
            "Trainable params: 400,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRx8EOvEoOdE",
        "colab_type": "text"
      },
      "source": [
        "Building a loss function and optimizer for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDjMHbKNoRaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K154DF8ToUIR",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Training the model over 20 epochs in batches of 32 using the train and validation datasets. The model's loss and accuracy will be monitored over 10,000 samples from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5icemnDiZlfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hub_layer.input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLep7hL2oXa1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa2392d8-2a6c-421a-e557-9f834c27598c"
      },
      "source": [
        "#history = model.fit(train_tfds.shuffle(10000).batch(512),epochs=20,validation_data=val_tfds.batch(32), verbose=1)\n",
        "\n",
        "history = model.fit(train_tfds.shuffle(10000).batch(512),\n",
        "                    epochs=20,\n",
        "                    validation_data=val_tfds.batch(512),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:Model was constructed with shape (None,) for input Tensor(\"keras_layer_19_input_1:0\", shape=(None,), dtype=string), but it was called on an input with incompatible shape (None, None).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['Unnamed: 0', 'benefitsReview', 'combinedReview', 'commentsReview', 'condition', 'effectiveness', 'rating', 'sideEffects', 'sideEffectsReview', 'urlDrugName'] which did not match any model input. They will be ignored by the model.\n",
            "  [n for n in tensors.keys() if n not in ref_input_names])\n",
            "WARNING:tensorflow:Model was constructed with shape (None,) for input Tensor(\"keras_layer_19_input_1:0\", shape=(None,), dtype=string), but it was called on an input with incompatible shape (None, None).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-216-cef365c380f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_tfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py:222 call  *\n        result = f()\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py:509 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:780 __call__\n        result = self._call(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:814 _call\n        results = self._stateful_fn(*args, **kwds)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2828 __call__\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3171 _maybe_define_function\n        *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2622 canonicalize_function_inputs\n        self._flat_input_signature)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2713 _convert_inputs_to_signature\n        format_error_message(inputs, input_signature))\n\n    ValueError: Python inputs incompatible with input_signature:\n      inputs: (\n        Tensor(\"sequential_4/Cast:0\", shape=(None, None), dtype=string))\n      input_signature: (\n        TensorSpec(shape=(None,), dtype=tf.string, name=None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ8HmKNGw8-d",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Evaluate the model performance on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzNKk3lFJaat",
        "colab_type": "text"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "This code was written using the Tensorflow tutorial documentation as a guide"
      ]
    }
  ]
}